{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "torch.manual_seed(0);\n",
    "gpu_boole = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 5000\n",
    "OUTPUT_DIR = './output'\n",
    "OUTPUT_DIR_TRAIN = os.path.join(OUTPUT_DIR, 'train')\n",
    "\n",
    "train_files = glob(os.path.join(OUTPUT_DIR_TRAIN, '**.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_list):\n",
    "    def load_into(_filename, _x):\n",
    "        with open(_filename, 'rb') as f:\n",
    "            audio_element = np.loadtxt(f, delimiter=\",\")\n",
    "            audio_element = audio_element.reshape(1,5000)\n",
    "            _x.append(audio_element)\n",
    "    x = []\n",
    "    count = 0\n",
    "    for filename in file_list:\n",
    "        if count < 10:\n",
    "            load_into(filename, x)\n",
    "        else:\n",
    "            break\n",
    "        #count += 1\n",
    "        \n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(782, 1, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data = get_data(train_files)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio2Mel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        sampling_rate=22050,\n",
    "        n_mel_channels=80,\n",
    "        mel_fmin=0.0,\n",
    "        mel_fmax=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ##############################################\n",
    "        # FFT Parameters                              #\n",
    "        ##############################################\n",
    "        window = torch.hann_window(win_length).float()\n",
    "        mel_basis = librosa_mel_fn(\n",
    "            sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax\n",
    "        )\n",
    "        mel_basis = torch.from_numpy(mel_basis).float()\n",
    "        self.register_buffer(\"mel_basis\", mel_basis)\n",
    "        self.register_buffer(\"window\", window)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "\n",
    "    def forward(self, audio):\n",
    "        p = (self.n_fft - self.hop_length) // 2\n",
    "        audio = F.pad(audio, (p, p), \"reflect\").squeeze(1)\n",
    "        fft = torch.stft(\n",
    "            audio,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window,\n",
    "            center=False,\n",
    "        )\n",
    "        real_part, imag_part = fft.unbind(-1)\n",
    "        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
    "        mel_output = torch.matmul(self.mel_basis, magnitude)\n",
    "        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n",
    "        return log_mel_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING ARCHITECTURE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, input_channel):\n",
    "        super(generator, self).__init__()\n",
    "        #TODO\n",
    "        self.reflPad1 = nn.ReflectionPad1d((3,3))\n",
    "        self.conv1 = nn.Conv1d(input_channel, 1024, kernel_size=1, stride=1)\n",
    "        self.act1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans1 = nn.ConvTranspose1d(1024,512, kernel_size=81, stride=16, padding=4)\n",
    "        self.act2 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans2 = nn.ConvTranspose1d(512,256, kernel_size=48, stride=8, padding=4)\n",
    "        self.act3 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans3 = nn.ConvTranspose1d(256,128, kernel_size=24, stride=4, padding=1)\n",
    "        self.act4 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans4 = nn.ConvTranspose1d(128,64,  kernel_size=18, stride=2, padding=1)\n",
    "        self.act5 = nn.LeakyReLU(0.2)\n",
    "        self.reflPad2 = nn.ReflectionPad1d((3,3))\n",
    "        \n",
    "        self.convTrans5 = nn.ConvTranspose1d(64,1, kernel_size=17, stride=1)\n",
    "        self.out = nn.Tanh()\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        #reflPad1_out = self.reflPad1(x)\n",
    "        conv1_out = self.conv1(x)\n",
    "        act1_out = self.act1(conv1_out)\n",
    "        \n",
    "        convTrans1_out = self.convTrans1(act1_out)\n",
    "        act2_out = self.act2(convTrans1_out)\n",
    "        \n",
    "        convTrans2_out = self.convTrans2(act2_out)\n",
    "        act3_out = self.act3(convTrans2_out)\n",
    "        \n",
    "        convTrans3_out = self.convTrans3(act3_out)\n",
    "        act4_out = self.act4(convTrans3_out)\n",
    "        \n",
    "        convTrans4_out = self.convTrans4(act4_out)\n",
    "        act5_out = self.act5(convTrans4_out)\n",
    "        reflPad2_out = self.reflPad2(act5_out)\n",
    "        \n",
    "        \n",
    "        convTrans5_out = self.convTrans5(reflPad2_out)\n",
    "        result = self.out(convTrans5_out)\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        #TODO\n",
    "        self.RefPad = nn.ReflectionPad1d((7,7))\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=15, stride=1)\n",
    "        self.act1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 64, kernel_size=8, stride=4, padding=20, groups=4)\n",
    "        self.act2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 256, kernel_size=16, stride=4, padding=20, groups=16)\n",
    "        self.act3 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(256, 1024, kernel_size=32, stride=4, padding=20, groups=64)\n",
    "        self.act4 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv5 = nn.Conv1d(1024, 1024, kernel_size=64, stride=4, padding=20, groups=256)\n",
    "        self.act5 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv6 = nn.Conv1d(1024, 1024, kernel_size=16, stride=1, padding=2)\n",
    "        self.act6 = nn.LeakyReLU(0.2)\n",
    "            \n",
    "        self.conv7 = nn.Conv1d(1024, 1, kernel_size=6, stride=1, padding=1)\n",
    "        # self.pool = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        reflPad_out = self.RefPad(x)\n",
    "        conv1_out = self.conv1(reflPad_out)\n",
    "        act1_out = self.act1(conv1_out)\n",
    "        \n",
    "        conv2_out = self.conv2(act1_out)\n",
    "        act2_out = self.act2(conv2_out)\n",
    "        \n",
    "        conv3_out = self.conv3(act2_out)\n",
    "        act3_out = self.act3(conv3_out)\n",
    "        \n",
    "        conv4_out = self.conv4(act3_out)\n",
    "        act4_out = self.act4(conv4_out)\n",
    "        \n",
    "        conv5_out = self.conv5(act4_out)\n",
    "        act5_out = self.act5(conv5_out)\n",
    "        \n",
    "        conv6_out = self.conv6(act5_out)\n",
    "        act6_out = self.act6(conv6_out)\n",
    "        \n",
    "        conv7_out = self.conv7(act6_out)\n",
    "        result = self.sig(conv7_out)\n",
    "        \n",
    "        return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator(\n",
      "  (reflPad1): ReflectionPad1d((3, 3))\n",
      "  (conv1): Conv1d(80, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (act1): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans1): ConvTranspose1d(1024, 512, kernel_size=(81,), stride=(16,), padding=(4,))\n",
      "  (act2): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans2): ConvTranspose1d(512, 256, kernel_size=(48,), stride=(8,), padding=(4,))\n",
      "  (act3): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans3): ConvTranspose1d(256, 128, kernel_size=(24,), stride=(4,), padding=(1,))\n",
      "  (act4): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans4): ConvTranspose1d(128, 64, kernel_size=(18,), stride=(2,), padding=(1,))\n",
      "  (act5): LeakyReLU(negative_slope=0.2)\n",
      "  (reflPad2): ReflectionPad1d((3, 3))\n",
      "  (convTrans5): ConvTranspose1d(64, 1, kernel_size=(17,), stride=(1,))\n",
      "  (out): Tanh()\n",
      ")\n",
      "discriminator(\n",
      "  (RefPad): ReflectionPad1d((7, 7))\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(15,), stride=(1,))\n",
      "  (act1): LeakyReLU(negative_slope=0.2)\n",
      "  (conv2): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(20,), groups=4)\n",
      "  (act2): LeakyReLU(negative_slope=0.2)\n",
      "  (conv3): Conv1d(64, 256, kernel_size=(16,), stride=(4,), padding=(20,), groups=16)\n",
      "  (act3): LeakyReLU(negative_slope=0.2)\n",
      "  (conv4): Conv1d(256, 1024, kernel_size=(32,), stride=(4,), padding=(20,), groups=64)\n",
      "  (act4): LeakyReLU(negative_slope=0.2)\n",
      "  (conv5): Conv1d(1024, 1024, kernel_size=(64,), stride=(4,), padding=(20,), groups=256)\n",
      "  (act5): LeakyReLU(negative_slope=0.2)\n",
      "  (conv6): Conv1d(1024, 1024, kernel_size=(16,), stride=(1,), padding=(2,))\n",
      "  (act6): LeakyReLU(negative_slope=0.2)\n",
      "  (conv7): Conv1d(1024, 1, kernel_size=(6,), stride=(1,), padding=(1,))\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Pass sr=22050, n_fft=1024, n_mels=80, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "tensor([[0.0187, 0.0181, 0.0092,  ..., 0.0345, 0.0434, 0.0234]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "[1/10] - loss_d: 99.372, loss_g: 0.000\n",
      "tensor([[0.0186, 0.0180, 0.0094,  ..., 0.0344, 0.0438, 0.0236]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "[2/10] - loss_d: 99.686, loss_g: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8e252fd26e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mD_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mD_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# TODO train generator G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training code:\n",
    "\n",
    "gpu_boole = torch.cuda.is_available()\n",
    "cnn_boole = True #set True for CNN reshaping\n",
    "\n",
    "#TODO tune the hyper parameter carefully to achieve a nash equilibrium\n",
    "#The initial hyper parameters are not ideal, you need to tune them to make things work.\n",
    "k=80 \n",
    "epochs = 10\n",
    "batch_size = 5\n",
    "lr_g = 0.2\n",
    "lr_d = 0.2\n",
    "train_interval = 10\n",
    "\n",
    "G = generator(k)\n",
    "D = discriminator()\n",
    "print(G)\n",
    "print(D)\n",
    "\n",
    "if gpu_boole:\n",
    "    G = G.cuda()\n",
    "    D = D.cuda()\n",
    "    \n",
    "#data loader:\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#D,G optimizers:\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_g)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_d)\n",
    "fft = Audio2Mel(n_mel_channels=k)\n",
    "\n",
    "#loss definition(s):\n",
    "BCELoss = nn.BCELoss()\n",
    "\n",
    "#training loop:\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "print(\"Training start!\")\n",
    "for epoch in range(epochs):\n",
    "    batch_number = 0\n",
    "    for x_ in train_loader:\n",
    "        batch_number += 1\n",
    "        #reshaping depending on your architecture class:\n",
    "        #if not cnn_boole:\n",
    "            #x_ = x_.view(batch_size,-1) #this reshape is needed for MLP class\n",
    "        if gpu_boole:\n",
    "            x_ = x_\n",
    "        \n",
    "        x_ = x_.float().cuda()\n",
    "        # print(\"x_: \", x_.shape)\n",
    "        # x_ = fft(x_).detach().cuda()\n",
    "        # print(\"s_\", x_.shape)\n",
    "            \n",
    "        mini_batch = x_.size()[0]\n",
    "        y_real_ = torch.ones(mini_batch)\n",
    "        y_fake_ = torch.zeros(mini_batch)\n",
    "        if gpu_boole:\n",
    "            y_real_ = y_real_.cuda()\n",
    "            y_fake_ = y_fake_.cuda()\n",
    "        z_ = torch.randn((mini_batch, k))\n",
    "        #print(\"PRIOR: \", z_.shape)\n",
    "        if cnn_boole:\n",
    "            z_ = z_.view(-1, k, 1) #needed for CNN        \n",
    "        if gpu_boole:\n",
    "            z_ = z_.cuda()\n",
    "            \n",
    "        #print(\"FAKE SEEDS: \", z_.shape)\n",
    "        # TODO train discriminator D\n",
    "        # Step 1 get prediction of D on real data x_ and calculate D_real_loss for real data\n",
    "        D_pred_real = D.forward(x_)\n",
    "        #print(\"DISCRIMINATOR PREDICTION REAL: \", D_pred_real.shape)\n",
    "        D_pred_real = torch.reshape(D_pred_real, (-1,))\n",
    "        D_real_loss = BCELoss(D_pred_real, y_real_)\n",
    "        \n",
    "        # Step 2 get prediction of D on fake data generated by generator based on z_\n",
    "        # and calculate D_fake_loss for fake data\n",
    "        fake_images = G.forward(z_)\n",
    "        #print(fake_images[0])\n",
    "        D_pred_fake = D.forward(fake_images)\n",
    "        #print(\"DISCRIMANTOR PREDICTION FAKE: \", D_pred_fake.shape)\n",
    "        D_pred_fake = torch.reshape(D_pred_fake, (-1,))\n",
    "        #print(\"D pred fake: \", D_pred_fake.shape)\n",
    "        #print(\"y_fake \", y_fake_.shape)\n",
    "        D_fake_loss = BCELoss(D_pred_fake, y_fake_)\n",
    "        \n",
    "        \n",
    "        # Step 3 calculate the overall loss for D and update weight. (we've done this for you)\n",
    "        D_train_loss = D_real_loss + D_fake_loss\n",
    "        D.zero_grad()\n",
    "        D_train_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        D_losses.append(D_train_loss.data.item())\n",
    "        \n",
    "        # TODO train generator G\n",
    "        # Step 0 think about the collapse problem we mentioned in lectures\n",
    "        # and how we deal with that. The hyperparameter train_interval might help.\n",
    "        if batch_number%train_interval!=0:\n",
    "            \n",
    "            # Step 1 calculate a new z_ and get prediction of fake data generated by \n",
    "            # generator based on z_\n",
    "            z_ = torch.randn((mini_batch, k))\n",
    "            if cnn_boole:\n",
    "                z_ = z_.view(-1, k, 1) #needed for CNN        \n",
    "            if gpu_boole:\n",
    "                z_ = z_.cuda()\n",
    "            fake_images = G.forward(z_)\n",
    "            G_pred_fake = D.forward(fake_images)\n",
    "            G_pred_fake = torch.reshape(G_pred_fake, (-1,))\n",
    "            G_train_loss = BCELoss(G_pred_fake, y_real_)\n",
    "\n",
    "            # Step 2 calculate the train loss for generator and update weight (we've done this for you)\n",
    "            G.zero_grad()\n",
    "            G_train_loss.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "            G_losses.append(G_train_loss.data.item())\n",
    "    print(fake_images[0])\n",
    "    print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), epochs, torch.mean(torch.FloatTensor(D_losses)),\n",
    "                                                                  torch.mean(torch.FloatTensor(G_losses))))\n",
    "\n",
    "#Plotting:\n",
    "\n",
    "#Losses:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(D_losses)\n",
    "plt.title(\"D Loss\")\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(G_losses)\n",
    "plt.title(\"G Loss\")\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
