{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0);\n",
    "gpu_boole = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, input_channel):\n",
    "        super(discriminator, self).__init__()\n",
    "        #TODO\n",
    "        self.reflPad1 = nn.ReflectionPad1d((3,3))\n",
    "        self.conv1 = nn.Conv1d(input_channel, 512, kernel_size=7, stride=1)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans1 = nn.ConvTranspose1d(512,256, kernel_size=16, stride=8, padding=4)\n",
    "        self.convTrans2 = nn.ConvTranspose1d(256,128, kernel_size=16, stride=8, padding=4)\n",
    "        self.convTrans3 = nn.ConvTranspose1d(128,64, kernel_size=4, stride=2, padding=1)\n",
    "        self.convTrans4 = nn.ConvTranspose1d(64,32,  kernel_size=4, stride=2, padding=1)\n",
    "        self.convTrans5 = nn.ConvTranspose1d(32,1, kernel_size=7, stride=1)\n",
    "        self.out = nn.Tanh()\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        #TODO\n",
    "        reflPad1_out = self.reflPad1(input)\n",
    "        conv1_out = self.conv1(reflPad1_out)\n",
    "        act_out = self.act(conv1_out)\n",
    "        \n",
    "        convTrans1_out = self.convTrans1(act_out)\n",
    "        act1_out = self.act(convTrans1_out)\n",
    "        \n",
    "        convTrans2_out = self.convTrans2(act1_out)\n",
    "        act2_out = self.act(convTrans2_out)\n",
    "        \n",
    "        convTrans3_out = self.convTrans3(act2_out)\n",
    "        act3_out = self.act(convTrans3_out)\n",
    "        \n",
    "        convTrans4_out = self.convTrans4(act3_out)\n",
    "        act4_out = self.act(convTrans4_out)\n",
    "        reflPad2_out = self.reflPad1(act4_out)\n",
    "        \n",
    "        \n",
    "        convTrans5_out = self.convTrans5(reflPad2_out)\n",
    "        result = self.out(convTrans5_out)\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        #TODO\n",
    "        self.RefPad = nn.ReflectionPad1d((3,3))\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=15, stride=1)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 64, kernel_size=41, stride=4, padding=20, groups=4)\n",
    "        self.conv3 = nn.Conv1d(64, 256, kernel_size=41, stride=4, padding=20, groups=16)\n",
    "        self.conv4 = nn.Conv1d(256, 1024, kernel_size=41, stride=4, padding=20, groups=16)\n",
    "        self.conv5 = nn.Conv1d(1024, 1024, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv6 = nn.Conv1d(1024, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        #TODO\n",
    "        reflPad_out = self.RefPad(input)\n",
    "        conv1_out = self.conv1(reflPad_out)\n",
    "        act1_out = self.act(conv1_out)\n",
    "        \n",
    "        conv2_out = self.conv1(act1_out)\n",
    "        act2_out = self.act(conv2_out)\n",
    "        \n",
    "        conv3_out = self.conv1(act2_out)\n",
    "        act3_out = self.act(conv3_out)\n",
    "        \n",
    "        conv4_out = self.conv1(act3_out)\n",
    "        act4_out = self.act(conv4_out)\n",
    "        \n",
    "        conv5_out = self.conv1(act4_out)\n",
    "        act5_out = self.act(conv5_out)\n",
    "        \n",
    "        conv6_out = self.conv1(act5_out)\n",
    "        resultt = self.pool(conv6_out)\n",
    "        \n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training code:\n",
    "\n",
    "gpu_boole = torch.cuda.is_available()\n",
    "cnn_boole = True #set True for CNN reshaping\n",
    "\n",
    "#TODO tune the hyper parameter carefully to achieve a nash equilibrium\n",
    "#The initial hyper parameters are not ideal, you need to tune them to make things work.\n",
    "k=100 \n",
    "epochs = 10\n",
    "batch_size = 32 \n",
    "lr_g = 0.2\n",
    "lr_d = 0.2\n",
    "train_interval = 10\n",
    "\n",
    "G = generator(k)\n",
    "D = discriminator()\n",
    "\n",
    "if gpu_boole:\n",
    "    G = G.cuda()\n",
    "    D = D.cuda()\n",
    "    \n",
    "#data loader:\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "#        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#D,G optimizers:\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_g)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_d)\n",
    "\n",
    "#loss definition(s):\n",
    "BCE_loss = nn.l1_loss()\n",
    "\n",
    "#training loop:\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "print(\"Training start!\")\n",
    "for epoch in range(epochs):\n",
    "    batch_number = 0\n",
    "    for x_, _ in train_loader:\n",
    "        batch_number += 1\n",
    "        #reshaping depending on your architecture class:\n",
    "        if not cnn_boole:\n",
    "            x_ = x_.view(batch_size,-1) #this reshape is needed for MLP class\n",
    "        if gpu_boole:\n",
    "            x_ = x_.cuda()\n",
    "        \n",
    "        \n",
    "        mini_batch = x_.size()[0]\n",
    "        y_real_ = torch.ones(mini_batch)\n",
    "        y_fake_ = torch.zeros(mini_batch)\n",
    "        if gpu_boole:\n",
    "            y_real_ = y_real_.cuda()\n",
    "            y_fake_ = y_fake_.cuda()\n",
    "        z_ = torch.randn((mini_batch, k))\n",
    "        if cnn_boole:\n",
    "            z_ = z_.view(-1, k, 1, 1) #needed for CNN        \n",
    "        if gpu_boole:\n",
    "            z_ = z_.cuda()\n",
    "        # TODO train discriminator D\n",
    "        # Step 1 get prediction of D on real data x_ and calculate D_real_loss for real data\n",
    "        D_pred_real = D.forward(x_)\n",
    "        D_pred_real = torch.reshape(D_pred_real, (-1,))\n",
    "        D_real_loss = BCE_loss(D_pred_real, y_real_)\n",
    "        \n",
    "        # Step 2 get prediction of D on fake data generated by generator based on z_\n",
    "        # and calculate D_fake_loss for fake data\n",
    "        fake_images = G.forward(z_)\n",
    "        D_pred_fake = D.forward(fake_images)\n",
    "        D_pred_fake = torch.reshape(D_pred_fake, (-1,))\n",
    "        D_fake_loss = BCE_loss(D_pred_fake, y_fake_)\n",
    "        \n",
    "        \n",
    "        # Step 3 calculate the overall loss for D and update weight. (we've done this for you)\n",
    "        D_train_loss = D_real_loss + D_fake_loss\n",
    "        D.zero_grad()\n",
    "        D_train_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        D_losses.append(D_train_loss.data.item())\n",
    "        \n",
    "        # TODO train generator G\n",
    "        # Step 0 think about the collapse problem we mentioned in lectures\n",
    "        # and how we deal with that. The hyperparameter train_interval might help.\n",
    "        if batch_number%train_interval!=0:\n",
    "            \n",
    "            # Step 1 calculate a new z_ and get prediction of fake data generated by \n",
    "            # generator based on z_\n",
    "            z_ = torch.randn((mini_batch, k))\n",
    "            if cnn_boole:\n",
    "                z_ = z_.view(-1, k, 1, 1) #needed for CNN        \n",
    "            if gpu_boole:\n",
    "                z_ = z_.cuda()\n",
    "            fake_images = G.forward(z_)\n",
    "            G_pred_fake = D.forward(fake_images)\n",
    "            G_pred_fake = torch.reshape(G_pred_fake, (-1,))\n",
    "            G_train_loss = BCE_loss(G_pred_fake, y_real_)\n",
    "\n",
    "            # Step 2 calculate the train loss for generator and update weight (we've done this for you)\n",
    "            G.zero_grad()\n",
    "            G_train_loss.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "            G_losses.append(G_train_loss.data.item())\n",
    "    \n",
    "    print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), epochs, torch.mean(torch.FloatTensor(D_losses)),\n",
    "                                                                  torch.mean(torch.FloatTensor(G_losses))))\n",
    "\n",
    "#Plotting:\n",
    "\n",
    "#Losses:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(D_losses)\n",
    "plt.title(\"D Loss\")\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(G_losses)\n",
    "plt.title(\"G Loss\")\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
