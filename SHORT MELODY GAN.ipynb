{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhLJS0Ix7RNN",
    "outputId": "3ca17736-8152-415a-d439-4d022702d994"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eQ-SaOZt56sZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "from datetime import datetime\n",
    "import time\n",
    "import scipy.io.wavfile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import csv\n",
    "import sys, wave\n",
    "\n",
    "torch.manual_seed(0);\n",
    "gpu_boole = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l4rf6Gq56sc"
   },
   "source": [
    "## GETTING DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_JztZA2_56se"
   },
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 12000\n",
    "OUTPUT_DIR = './mel_output/'\n",
    "OUTPUT_DIR_TRAIN = os.path.join(OUTPUT_DIR, 'train')\n",
    "\n",
    "train_files = glob(os.path.join(OUTPUT_DIR_TRAIN, '**.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bifLDvIi_NW-"
   },
   "outputs": [],
   "source": [
    "# shows the sound waves\n",
    "def visualize_wav(path: str):\n",
    "   \n",
    "    # reading the audio file\n",
    "    raw = wave.open(path)\n",
    "     \n",
    "    # reads all the frames\n",
    "    # -1 indicates all or max frames\n",
    "    signal = raw.readframes(-1)\n",
    "    signal = np.frombuffer(signal, dtype =\"int16\")\n",
    "     \n",
    "    # gets the frame rate\n",
    "    f_rate = raw.getframerate()\n",
    " \n",
    "    # to Plot the x-axis in seconds\n",
    "    # you need get the frame rate\n",
    "    # and divide by size of your signal\n",
    "    # to create a Time Vector\n",
    "    # spaced linearly with the size\n",
    "    # of the audio file\n",
    "    time = np.linspace(\n",
    "        0, # start\n",
    "        len(signal) / f_rate,\n",
    "        num = len(signal)\n",
    "    )\n",
    " \n",
    "    # using matplotlib to plot\n",
    "    # creates a new figure\n",
    "    plt.figure(1)\n",
    "     \n",
    "    # title of the plot\n",
    "    plt.title(\"Sound Wave\")\n",
    "     \n",
    "    # label of x-axis\n",
    "    plt.xlabel(\"Time\")\n",
    "    \n",
    "    # actual plotting\n",
    "    plt.plot(time, signal)\n",
    "     \n",
    "    # shows the plot\n",
    "    # in new window\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3TmZpbLN56sf"
   },
   "outputs": [],
   "source": [
    "def get_data(file_list):\n",
    "    def load_into(_filename, _x):\n",
    "        with open(_filename, 'rb') as f:\n",
    "            audio_element = np.loadtxt(f, delimiter=\",\")\n",
    "            #print(audio_element)\n",
    "            audio_element = audio_element.reshape(1,12000)\n",
    "            _x.append(audio_element)\n",
    "    x = []\n",
    "    count = 0\n",
    "    for filename in file_list:\n",
    "        if count < 10:\n",
    "            load_into(filename, x)\n",
    "        else:\n",
    "            break\n",
    "        #count += 1\n",
    "        \n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xd-m2GEf56sg",
    "outputId": "ca14b054-b3fb-4d2d-af8e-9dcf7ce302b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226, 1, 12000)\n"
     ]
    }
   ],
   "source": [
    "train_data = get_data(train_files)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a1dPJkC856si"
   },
   "outputs": [],
   "source": [
    "class Audio2Mel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        sampling_rate=22050,\n",
    "        n_mel_channels=80,\n",
    "        mel_fmin=0.0,\n",
    "        mel_fmax=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ##############################################\n",
    "        # FFT Parameters                              #\n",
    "        ##############################################\n",
    "        window = torch.hann_window(win_length).float()\n",
    "        mel_basis = librosa_mel_fn(\n",
    "            sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax\n",
    "        )\n",
    "        mel_basis = torch.from_numpy(mel_basis).float()\n",
    "        self.register_buffer(\"mel_basis\", mel_basis)\n",
    "        self.register_buffer(\"window\", window)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "\n",
    "    def forward(self, audio):\n",
    "        p = (self.n_fft - self.hop_length) // 2\n",
    "        audio = F.pad(audio, (p, p), \"reflect\").squeeze(1)\n",
    "        fft = torch.stft(\n",
    "            audio,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window,\n",
    "            center=False,\n",
    "        )\n",
    "        real_part, imag_part = fft.unbind(-1)\n",
    "        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n",
    "        mel_output = torch.matmul(self.mel_basis, magnitude)\n",
    "        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n",
    "        return log_mel_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h__Z-glN56sk"
   },
   "source": [
    "## CREATING ARCHITECTURE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ELzoHLEj56sl"
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, input_channel):\n",
    "        super(generator, self).__init__()\n",
    "        #TODO\n",
    "        self.reflPad1 = nn.ReflectionPad1d((3,3))\n",
    "        self.conv1 = nn.Conv1d(input_channel, 1024, kernel_size=1, stride=1)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.act1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans1 = nn.ConvTranspose1d(1024,512, kernel_size=180, stride=16, padding=4)\n",
    "        self.act2 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans2 = nn.ConvTranspose1d(512,256, kernel_size=120, stride=8, padding=4)\n",
    "        self.act3 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans3 = nn.ConvTranspose1d(256,128, kernel_size=70, stride=4, padding=1)\n",
    "        self.act4 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.convTrans4 = nn.ConvTranspose1d(128,64,  kernel_size=23, stride=2, padding=1)\n",
    "        self.act5 = nn.LeakyReLU(0.2)\n",
    "        self.reflPad2 = nn.ReflectionPad1d((3,3))\n",
    "        \n",
    "        self.convTrans5 = nn.ConvTranspose1d(64,1, kernel_size=8, stride=1)\n",
    "        self.out = nn.Tanh()\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            nn.normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        #reflPad1_out = self.reflPad1(x)\n",
    "        conv1_out = self.conv1(x)\n",
    "        act1_out = self.act1(conv1_out)\n",
    "        act1_out = self.dropout1(act1_out)\n",
    "        \n",
    "        convTrans1_out = self.convTrans1(act1_out)\n",
    "        act2_out = self.act2(convTrans1_out)\n",
    "        act2_out = self.dropout1(act2_out)\n",
    "        \n",
    "        convTrans2_out = self.convTrans2(act2_out)\n",
    "        act3_out = self.act3(convTrans2_out)\n",
    "        act3_out = self.dropout1(act3_out)\n",
    "        \n",
    "        convTrans3_out = self.convTrans3(act3_out)\n",
    "        act4_out = self.act4(convTrans3_out)\n",
    "        act4_out = self.dropout1(act4_out)\n",
    "        \n",
    "        convTrans4_out = self.convTrans4(act4_out)\n",
    "        act5_out = self.act5(convTrans4_out)\n",
    "        act5_out = self.dropout1(act5_out)\n",
    "        reflPad2_out = self.reflPad2(act5_out)\n",
    "        \n",
    "        convTrans5_out = self.convTrans5(reflPad2_out)\n",
    "        result = self.out(convTrans5_out)\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z4xS2yHv56sn"
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        #TODO\n",
    "        self.RefPad = nn.ReflectionPad1d((7,7))\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=15, stride=1)\n",
    "        self.act1 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32, 128, kernel_size=16, stride=4, padding=20, groups=4)\n",
    "        self.act2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 512, kernel_size=24, stride=4, padding=20, groups=16)\n",
    "        self.act3 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(512, 1024, kernel_size=32, stride=4, padding=20, groups=64)\n",
    "        self.act4 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv5 = nn.Conv1d(1024, 1024, kernel_size=64, stride=4, padding=20, groups=256)\n",
    "        self.act5 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv6 = nn.Conv1d(1024, 1024, kernel_size=36, stride=1, padding=2)\n",
    "        self.act6 = nn.LeakyReLU(0.2)\n",
    "            \n",
    "        self.conv7 = nn.Conv1d(1024, 1, kernel_size=14, stride=1, padding=1)\n",
    "        # self.pool = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            nn.normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        reflPad_out = self.RefPad(x)\n",
    "        conv1_out = self.conv1(reflPad_out)\n",
    "        act1_out = self.act1(conv1_out)\n",
    "        \n",
    "        conv2_out = self.conv2(act1_out)\n",
    "        act2_out = self.act2(conv2_out)\n",
    "        \n",
    "        conv3_out = self.conv3(act2_out)\n",
    "        act3_out = self.act3(conv3_out)\n",
    "        \n",
    "        conv4_out = self.conv4(act3_out)\n",
    "        act4_out = self.act4(conv4_out)\n",
    "        \n",
    "        conv5_out = self.conv5(act4_out)\n",
    "        act5_out = self.act5(conv5_out)\n",
    "        \n",
    "        conv6_out = self.conv6(act5_out)\n",
    "        act6_out = self.act6(conv6_out)\n",
    "        \n",
    "        conv7_out = self.conv7(act6_out)\n",
    "        result = self.sig(conv7_out)\n",
    "        \n",
    "        return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(file_path, sampling_rate, audio):\n",
    "    \"\"\"Helper function to save sample\n",
    "\n",
    "    Args:\n",
    "        file_path (str or pathlib.Path): save file path\n",
    "        sampling_rate (int): sampling rate of audio (usually 22050)\n",
    "        audio (torch.FloatTensor): torch array containing audio in [-1, 1]\n",
    "    \"\"\"\n",
    "    audio = (audio.numpy() * 32768).astype(\"int16\")\n",
    "    scipy.io.wavfile.write(file_path, sampling_rate, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaBgbHtA56so"
   },
   "source": [
    "## TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jtSODNc_56sp",
    "outputId": "c953bc6a-5e9e-4f5a-e70c-a268b416fc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator(\n",
      "  (reflPad1): ReflectionPad1d((3, 3))\n",
      "  (conv1): Conv1d(20, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (act1): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans1): ConvTranspose1d(1024, 512, kernel_size=(180,), stride=(16,), padding=(4,))\n",
      "  (act2): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans2): ConvTranspose1d(512, 256, kernel_size=(120,), stride=(8,), padding=(4,))\n",
      "  (act3): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans3): ConvTranspose1d(256, 128, kernel_size=(70,), stride=(4,), padding=(1,))\n",
      "  (act4): LeakyReLU(negative_slope=0.2)\n",
      "  (convTrans4): ConvTranspose1d(128, 64, kernel_size=(23,), stride=(2,), padding=(1,))\n",
      "  (act5): LeakyReLU(negative_slope=0.2)\n",
      "  (reflPad2): ReflectionPad1d((3, 3))\n",
      "  (convTrans5): ConvTranspose1d(64, 1, kernel_size=(8,), stride=(1,))\n",
      "  (out): Tanh()\n",
      ")\n",
      "discriminator(\n",
      "  (RefPad): ReflectionPad1d((7, 7))\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(15,), stride=(1,))\n",
      "  (act1): LeakyReLU(negative_slope=0.2)\n",
      "  (conv2): Conv1d(32, 128, kernel_size=(16,), stride=(4,), padding=(20,), groups=4)\n",
      "  (act2): LeakyReLU(negative_slope=0.2)\n",
      "  (conv3): Conv1d(128, 512, kernel_size=(24,), stride=(4,), padding=(20,), groups=16)\n",
      "  (act3): LeakyReLU(negative_slope=0.2)\n",
      "  (conv4): Conv1d(512, 1024, kernel_size=(32,), stride=(4,), padding=(20,), groups=64)\n",
      "  (act4): LeakyReLU(negative_slope=0.2)\n",
      "  (conv5): Conv1d(1024, 1024, kernel_size=(64,), stride=(4,), padding=(20,), groups=256)\n",
      "  (act5): LeakyReLU(negative_slope=0.2)\n",
      "  (conv6): Conv1d(1024, 1024, kernel_size=(36,), stride=(1,), padding=(2,))\n",
      "  (act6): LeakyReLU(negative_slope=0.2)\n",
      "  (conv7): Conv1d(1024, 1, kernel_size=(14,), stride=(1,), padding=(1,))\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Pass sr=22050, n_fft=1024, n_mels=20, fmin=0.0, fmax=None as keyword args. From version 0.10 passing these as positional arguments will result in an error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "saving model and example...\n"
     ]
    }
   ],
   "source": [
    "#Training code:\n",
    "\n",
    "gpu_boole = torch.cuda.is_available()\n",
    "cnn_boole = True #set True for CNN reshaping\n",
    "\n",
    "#TODO tune the hyper parameter carefully to achieve a nash equilibrium\n",
    "#The initial hyper parameters are not ideal, you need to tune them to make things work.\n",
    "k=20\n",
    "epochs = 150\n",
    "batch_size = 1\n",
    "lr_g = 0.0001\n",
    "lr_d = 0.00002\n",
    "train_interval = 4\n",
    "save_interval = 1200\n",
    "best_model = 999999\n",
    "\n",
    "\n",
    "G = generator(k)\n",
    "D = discriminator()\n",
    "print(G)\n",
    "print(D)\n",
    "\n",
    "if gpu_boole:\n",
    "    G = G.cuda()\n",
    "    D = D.cuda()\n",
    "    \n",
    "#data loader:\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#D,G optimizers:\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_g)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_d)\n",
    "fft = Audio2Mel(n_mel_channels=k)\n",
    "\n",
    "#loss definition(s):\n",
    "BCELoss = nn.BCELoss()\n",
    "\n",
    "#training loop:\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "print(\"Training start!\")\n",
    "for epoch in range(epochs):\n",
    "    batch_number = 0\n",
    "    time1 = time.time()\n",
    "    steps = 0\n",
    "    for x_ in train_loader:\n",
    "        batch_number += 1\n",
    "        # print(steps)\n",
    "        #reshaping depending on your architecture class:\n",
    "        #if not cnn_boole:\n",
    "            #x_ = x_.view(batch_size,-1) #this reshape is needed for MLP class\n",
    "        if gpu_boole:\n",
    "            x_ = x_\n",
    "        \n",
    "        x_ = x_.float().cuda()\n",
    "        # print(\"x_: \", x_.shape)\n",
    "        # x_ = fft(x_).detach().cuda()\n",
    "        # print(\"s_\", x_.shape)\n",
    "            \n",
    "        #Visualize the first data input\n",
    "        # if batch_number == 1:\n",
    "        #  print(x_[0])\n",
    "        #  plt.plot([i for i, _ in enumerate(x_[0].cpu().detach().numpy()[0])], x_[0].cpu().detach().numpy()[0], color=\"blue\")\n",
    "        #  plt.xlabel('Time')\n",
    "        #  plt.ylabel('Wave Values')\n",
    "        #  plt.title('Train Sample')\n",
    "        #  plt.xticks([i for i in range(len(x_[0].cpu().detach().numpy()[0]))])\n",
    "        #  plt.show()\n",
    "        mini_batch = x_.size()[0]\n",
    "        y_real_ = torch.ones(mini_batch)\n",
    "        y_fake_ = torch.zeros(mini_batch)\n",
    "        if gpu_boole:\n",
    "            y_real_ = y_real_.cuda()\n",
    "            y_fake_ = y_fake_.cuda()\n",
    "        z_ = torch.randn((mini_batch, k))\n",
    "        #print(\"PRIOR: \", z_.shape)\n",
    "        if cnn_boole:\n",
    "            z_ = z_.view(-1, k, 1) #needed for CNN        \n",
    "        if gpu_boole:\n",
    "            z_ = z_.cuda()\n",
    "            \n",
    "        #print(\"FAKE SEEDS: \", z_.shape)\n",
    "        # TODO train discriminator D\n",
    "        # Step 1 get prediction of D on real data x_ and calculate D_real_loss for real data\n",
    "        #print(\"WTF\")\n",
    "        D_pred_real = D.forward(x_)\n",
    "        #print(\"REAL PREDICTIONS SHAPE: \", D_pred_real.shape)\n",
    "        D_pred_real = torch.reshape(D_pred_real, (-1,))\n",
    "        D_real_loss = BCELoss(D_pred_real, y_real_)\n",
    "        \n",
    "        # Step 2 get prediction of D on fake data generated by generator based on z_\n",
    "        # and calculate D_fake_loss for fake data\n",
    "        fake_images = G.forward(z_)\n",
    "        #print(\"GENERATOR OUTPUT SHAPE: \", fake_images.shape)\n",
    "        D_pred_fake = D.forward(fake_images)\n",
    "        #print(\"DISCRIMANTOR PREDICTION FAKE: \", D_pred_fake.shape)\n",
    "        D_pred_fake = torch.reshape(D_pred_fake, (-1,))\n",
    "        #print(\"D pred fake: \", D_pred_fake.shape)\n",
    "        #print(\"y_fake \", y_fake_.shape)\n",
    "        D_fake_loss = BCELoss(D_pred_fake, y_fake_)\n",
    "        \n",
    "        \n",
    "        # Step 3 calculate the overall loss for D and update weight. (we've done this for you)\n",
    "        D_train_loss = D_real_loss + D_fake_loss\n",
    "        D.zero_grad()\n",
    "        D_train_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        D_losses.append(D_train_loss.data.item())\n",
    "        \n",
    "        # TODO train generator G\n",
    "        # Step 0 think about the collapse problem we mentioned in lectures\n",
    "        # and how we deal with that. The hyperparameter train_interval might help.\n",
    "        if batch_number % train_interval!=0:\n",
    "            \n",
    "            # Step 1 calculate a new z_ and get prediction of fake data generated by \n",
    "            # generator based on z_\n",
    "            z_ = torch.randn((mini_batch, k))\n",
    "            if cnn_boole:\n",
    "                z_ = z_.view(-1, k, 1) #needed for CNN        \n",
    "            if gpu_boole:\n",
    "                z_ = z_.cuda()\n",
    "            fake_images = G.forward(z_)\n",
    "            G_pred_fake = D.forward(fake_images)\n",
    "            G_pred_fake = torch.reshape(G_pred_fake, (-1,))\n",
    "            G_train_loss = BCELoss(G_pred_fake, y_real_)\n",
    "\n",
    "            # Step 2 calculate the train loss for generator and update weight (we've done this for you)\n",
    "            G.zero_grad()\n",
    "            G_train_loss.backward()\n",
    "            G_optimizer.step()\n",
    "\n",
    "            G_losses.append(G_train_loss.data.item())\n",
    "    # print(fake_images[0].cpu().detach().numpy()[0])\n",
    "    # plt.plot([idx for idx, _ in enumerate(fake_images[0].cpu().detach().numpy()[0])], fake_images[0].cpu().detach().numpy()[0], color=\"red\")\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Wave Value')\n",
    "    # plt.title('Generated Sample')\n",
    "    # plt.xticks([i for i in range(len(fake_images[0].cpu().detach().numpy()[0]))])\n",
    "    # plt.show()\n",
    "        if steps % save_interval == 0:\n",
    "            print(\"saving model and example...\")\n",
    "            st = time.time()\n",
    "            with torch.no_grad():\n",
    "                pred_audio = fake_images[0]\n",
    "                with open((\"./generated_melody/generated_%d.csv\" % epoch), 'w') as w:\n",
    "                    mywriter = csv.writer(w, delimiter=\",\")\n",
    "                    mywriter.writerows(pred_audio.cpu().numpy())\n",
    "\n",
    "            torch.save(G.state_dict(), \"./saved_mel_models/generator_net.pt\")\n",
    "            torch.save(G_optimizer.state_dict, \"./saved_mel_models/generator_opt.pt\")\n",
    "\n",
    "            torch.save(D.state_dict(), \"./saved_mel_models/discriminator_net.pt\")\n",
    "            torch.save(D_optimizer.state_dict, \"./saved_mel_models/discriminator_opt.pt\")\n",
    "\n",
    "            if torch.mean(torch.FloatTensor(D_losses) + torch.mean(torch.FloatTensor(G_losses))) <= best_model:\n",
    "                best_model = torch.mean(torch.FloatTensor(D_losses) + torch.mean(torch.FloatTensor(G_losses)))\n",
    "                torch.save(G.state_dict(), \"./saved_mel_models/best_generator_net.pt\")\n",
    "                torch.save(G_optimizer.state_dict, \"./saved_mel_models/best_generator_opt.pt\")\n",
    "\n",
    "                torch.save(D.state_dict(), \"./saved_mel_models/best_discriminator_net.pt\")\n",
    "                torch.save(D_optimizer.state_dict, \"./saved_mel_models/best_discriminator_opt.pt\")\n",
    "        steps += 1\n",
    "    \n",
    "    print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), epochs, torch.mean(torch.FloatTensor(D_losses[-batch_number:])),\n",
    "                                      torch.mean(torch.FloatTensor(G_losses[-(batch_number - batch_number // train_interval):]))))\n",
    "    \n",
    "    time2 = time.time() #timekeeping\n",
    "    print('Elapsed time for epoch:',(time2 - time1)/60,'minutes')\n",
    "    print('ETA of completion:',(time2 - time1)*(epochs - epoch - 1)/60,'minutes')\n",
    "    print()\n",
    "    \n",
    "#Plotting:\n",
    "\n",
    "#Losses:\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(D_losses)\n",
    "plt.title(\"D Loss\")\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(G_losses)\n",
    "plt.title(\"G Loss\")\n",
    "plt.xlabel(\"Batch number\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oicXdtK456sq"
   },
   "outputs": [],
   "source": [
    "z_ = torch.randn((mini_batch, k))\n",
    "    if cnn_boole:\n",
    "        z_ = z_.view(-1, k, 1) #needed for CNN        \n",
    "    if gpu_boole:\n",
    "        z_ = z_.cuda()\n",
    "    fake_images = G.forward(z_)\n",
    "\n",
    "\n",
    "#visualize_wav(f\"./drive/MyDrive/DL_FINAL_PROJECT/sample_outputs/sample_{datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r-nsbls56sr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHKY8LNc56sr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of HEARTBEAT_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
